# èŠå¤©æœåŠ¡æ¨¡å—
# æä¾›AIå›å¤ç”ŸæˆåŠŸèƒ½ï¼Œæ”¯æŒæ™ºèƒ½ä½“å’Œæ¨¡å‹ä¸¤ç§ç±»å‹
# åŒ…å«åŒæ­¥å’Œæµå¼ä¸¤ç§ç”Ÿæˆæ¨¡å¼

import json
import time
from typing import Dict, Any, Generator
from app.services.database import get_db
from app.services.ollama import OllamaService
from app.services.activity_service import activity_service
from bson import ObjectId
from datetime import datetime

class ChatService:
    """
    èŠå¤©æœåŠ¡ç±»
    è´Ÿè´£å¤„ç†AIå›å¤ç”Ÿæˆï¼Œæ”¯æŒæ™ºèƒ½ä½“å’Œæ¨¡å‹ä¸¤ç§ç±»å‹
    """
    
    def __init__(self):
        """åˆå§‹åŒ–èŠå¤©æœåŠ¡"""
        self.db = get_db()
    
    def generate_response_sync(self, conversation_id: str, agent: Dict[str, Any], user_message: str) -> str:
        """
        åŒæ­¥ç”Ÿæˆæ™ºèƒ½ä½“å›å¤
        
        Args:
            conversation_id: å¯¹è¯ID
            agent: æ™ºèƒ½ä½“é…ç½®ä¿¡æ¯
            user_message: ç”¨æˆ·æ¶ˆæ¯
            
        Returns:
            str: AIå›å¤å†…å®¹
        """
        try:
            print(f"ğŸ¤– å¼€å§‹ç”Ÿæˆæ™ºèƒ½ä½“å›å¤ - æ™ºèƒ½ä½“: {agent.get('name', 'æœªçŸ¥')}")
            
            # è·å–å¯¹è¯å†å²
            messages = self._get_conversation_history(conversation_id)
            
            # æ„å»ºç³»ç»Ÿæç¤ºè¯
            system_prompt = self._build_agent_system_prompt(agent)
            
            # æ„å»ºç”¨æˆ·æ¶ˆæ¯
            user_prompt = self._build_user_prompt(user_message, messages)
            
            # è°ƒç”¨Ollamaç”Ÿæˆå›å¤
            response = self._call_ollama_for_agent(agent, system_prompt, user_prompt)
            
            print(f"âœ… æ™ºèƒ½ä½“å›å¤ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(response)}")
            return response
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ™ºèƒ½ä½“å›å¤å¤±è´¥: {str(e)}")
            raise e
    
    def generate_model_response_sync(self, conversation_id: str, model: Dict[str, Any], user_message: str) -> str:
        """
        åŒæ­¥ç”Ÿæˆæ¨¡å‹å›å¤
        
        Args:
            conversation_id: å¯¹è¯ID
            model: æ¨¡å‹é…ç½®ä¿¡æ¯
            user_message: ç”¨æˆ·æ¶ˆæ¯
            
        Returns:
            str: AIå›å¤å†…å®¹
        """
        try:
            print(f"ğŸ¤– å¼€å§‹ç”Ÿæˆæ¨¡å‹å›å¤ - æ¨¡å‹: {model.get('name', 'æœªçŸ¥')}")
            
            # è·å–å¯¹è¯å†å²
            messages = self._get_conversation_history(conversation_id)
            
            # æ„å»ºç”¨æˆ·æ¶ˆæ¯
            user_prompt = self._build_user_prompt(user_message, messages)
            
            # è°ƒç”¨Ollamaç”Ÿæˆå›å¤
            response = self._call_ollama_for_model(model, user_prompt)
            
            print(f"âœ… æ¨¡å‹å›å¤ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(response)}")
            return response
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ¨¡å‹å›å¤å¤±è´¥: {str(e)}")
            raise e
    
    def stream_response(self, conversation_id: str, agent: Dict[str, Any], user_message: str) -> Generator[str, None, None]:
        """
        æµå¼ç”Ÿæˆæ™ºèƒ½ä½“å›å¤
        
        Args:
            conversation_id: å¯¹è¯ID
            agent: æ™ºèƒ½ä½“é…ç½®ä¿¡æ¯
            user_message: ç”¨æˆ·æ¶ˆæ¯
            
        Yields:
            str: æµå¼å›å¤ç‰‡æ®µ
        """
        try:
            print(f"ğŸ¤– å¼€å§‹æµå¼ç”Ÿæˆæ™ºèƒ½ä½“å›å¤ - æ™ºèƒ½ä½“: {agent.get('name', 'æœªçŸ¥')}")
            
            # è·å–å¯¹è¯å†å²
            messages = self._get_conversation_history(conversation_id)
            
            # æ„å»ºç³»ç»Ÿæç¤ºè¯
            system_prompt = self._build_agent_system_prompt(agent)
            
            # æ„å»ºç”¨æˆ·æ¶ˆæ¯
            user_prompt = self._build_user_prompt(user_message, messages)
            
            # è°ƒç”¨Ollamaæµå¼ç”Ÿæˆå›å¤
            for chunk in self._call_ollama_stream_for_agent(agent, system_prompt, user_prompt):
                yield chunk
                
            print(f"âœ… æ™ºèƒ½ä½“æµå¼å›å¤ç”Ÿæˆå®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æµå¼ç”Ÿæˆæ™ºèƒ½ä½“å›å¤å¤±è´¥: {str(e)}")
            yield f"æŠ±æ­‰ï¼Œæ™ºèƒ½ä½“ {agent.get('name', 'æœªçŸ¥')} æš‚æ—¶æ— æ³•å“åº”ï¼š{str(e)}"
    
    def stream_model_response(self, conversation_id: str, model: Dict[str, Any], user_message: str, show_thinking: bool = False) -> Generator[str, None, None]:
        """
        æµå¼ç”Ÿæˆæ¨¡å‹å›å¤
        
        Args:
            conversation_id: å¯¹è¯ID
            model: æ¨¡å‹é…ç½®ä¿¡æ¯
            user_message: ç”¨æˆ·æ¶ˆæ¯
            show_thinking: æ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
            
        Yields:
            str: æµå¼å›å¤ç‰‡æ®µ
        """
        try:
            print(f"ğŸ¤– å¼€å§‹æµå¼ç”Ÿæˆæ¨¡å‹å›å¤ - æ¨¡å‹: {model.get('name', 'æœªçŸ¥')}")
            
            # è·å–å¯¹è¯å†å²
            messages = self._get_conversation_history(conversation_id)
            
            # æ„å»ºç”¨æˆ·æ¶ˆæ¯
            user_prompt = self._build_user_prompt(user_message, messages)
            
            # è°ƒç”¨Ollamaæµå¼ç”Ÿæˆå›å¤
            for chunk in self._call_ollama_stream_for_model(model, user_prompt, show_thinking):
                yield chunk
                
            print(f"âœ… æ¨¡å‹æµå¼å›å¤ç”Ÿæˆå®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æµå¼ç”Ÿæˆæ¨¡å‹å›å¤å¤±è´¥: {str(e)}")
            yield f"æŠ±æ­‰ï¼Œæ¨¡å‹ {model.get('name', 'æœªçŸ¥')} æš‚æ—¶æ— æ³•å“åº”ï¼š{str(e)}"
    
    def _get_conversation_history(self, conversation_id: str) -> list:
        """
        è·å–å¯¹è¯å†å²æ¶ˆæ¯
        
        Args:
            conversation_id: å¯¹è¯ID
            
        Returns:
            list: æ¶ˆæ¯å†å²åˆ—è¡¨
        """
        try:
            # æŸ¥è¯¢æœ€è¿‘çš„æ¶ˆæ¯å†å²ï¼ˆé™åˆ¶æ•°é‡ä»¥é¿å…tokenè¿‡å¤šï¼‰
            messages = list(self.db.messages.find({
                'conversation_id': ObjectId(conversation_id)
            }).sort('created_at', -1).limit(10))
            
            # æŒ‰æ—¶é—´æ­£åºæ’åˆ—
            messages.reverse()
            
            print(f"ğŸ“ è·å–åˆ° {len(messages)} æ¡å†å²æ¶ˆæ¯")
            return messages
            
        except Exception as e:
            print(f"âŒ è·å–å¯¹è¯å†å²å¤±è´¥: {str(e)}")
            return []
    
    def _build_agent_system_prompt(self, agent: Dict[str, Any]) -> str:
        """
        æ„å»ºæ™ºèƒ½ä½“ç³»ç»Ÿæç¤ºè¯
        
        Args:
            agent: æ™ºèƒ½ä½“é…ç½®ä¿¡æ¯
            
        Returns:
            str: ç³»ç»Ÿæç¤ºè¯
        """
        try:
            # åŸºç¡€ç³»ç»Ÿæç¤ºè¯
            system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªåä¸º"{agent.get('name', 'AIåŠ©æ‰‹')}"çš„æ™ºèƒ½ä½“ã€‚

{agent.get('description', 'æˆ‘æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·è§£å†³å„ç§é—®é¢˜ã€‚')}

è¯·éµå¾ªä»¥ä¸‹æŒ‡å¯¼åŸåˆ™ï¼š
1. å§‹ç»ˆä¿æŒå‹å¥½ã€ä¸“ä¸šå’Œæœ‰ç”¨çš„æ€åº¦
2. æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›å‡†ç¡®ã€ç›¸å…³çš„å›ç­”
3. å¦‚æœä¸ç¡®å®šç­”æ¡ˆï¼Œè¯·è¯šå®è¯´æ˜
4. ä½¿ç”¨æ¸…æ™°ã€æ˜“æ‡‚çš„è¯­è¨€
5. ä¿æŒå¯¹è¯çš„è‡ªç„¶æµç•…

ç°åœ¨å¼€å§‹ä¸ç”¨æˆ·å¯¹è¯ã€‚"""

            # å¦‚æœæœ‰è‡ªå®šä¹‰çš„ç³»ç»Ÿæç¤ºè¯ï¼Œä½¿ç”¨è‡ªå®šä¹‰çš„
            if agent.get('system_prompt'):
                system_prompt = agent['system_prompt']
            
            print(f"ğŸ”§ æ„å»ºæ™ºèƒ½ä½“ç³»ç»Ÿæç¤ºè¯: {agent.get('name', 'æœªçŸ¥')}")
            return system_prompt
            
        except Exception as e:
            print(f"âŒ æ„å»ºæ™ºèƒ½ä½“ç³»ç»Ÿæç¤ºè¯å¤±è´¥: {str(e)}")
            return "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"
    
    def _build_user_prompt(self, user_message: str, messages: list) -> str:
        """
        æ„å»ºç”¨æˆ·æ¶ˆæ¯æç¤ºè¯
        
        Args:
            user_message: å½“å‰ç”¨æˆ·æ¶ˆæ¯
            messages: å†å²æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            str: å®Œæ•´çš„ç”¨æˆ·æç¤ºè¯
        """
        try:
            # æ„å»ºå¯¹è¯å†å²
            conversation_history = ""
            for msg in messages:
                if msg['type'] == 'user':
                    conversation_history += f"ç”¨æˆ·: {msg['content']}\n"
                elif msg['type'] == 'assistant':
                    conversation_history += f"åŠ©æ‰‹: {msg['content']}\n"
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            full_prompt = f"{conversation_history}ç”¨æˆ·: {user_message}\nåŠ©æ‰‹:"
            
            print(f"ğŸ“ æ„å»ºç”¨æˆ·æç¤ºè¯ï¼Œå†å²æ¶ˆæ¯æ•°: {len(messages)}")
            return full_prompt
            
        except Exception as e:
            print(f"âŒ æ„å»ºç”¨æˆ·æç¤ºè¯å¤±è´¥: {str(e)}")
            return f"ç”¨æˆ·: {user_message}\nåŠ©æ‰‹:"
    
    def _call_ollama_for_agent(self, agent: Dict[str, Any], system_prompt: str, user_prompt: str) -> str:
        """
        è°ƒç”¨Ollamaç”Ÿæˆæ™ºèƒ½ä½“å›å¤
        
        Args:
            agent: æ™ºèƒ½ä½“é…ç½®ä¿¡æ¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            
        Returns:
            str: AIå›å¤å†…å®¹
        """
        try:
            # è·å–OllamaæœåŠ¡å™¨åœ°å€
            server_url = agent.get('server_url', 'http://localhost:11434')
            print(f"ğŸ”§ è°ƒç”¨Ollamaç”Ÿæˆæ™ºèƒ½ä½“å›å¤ - æœåŠ¡å™¨: {server_url}")
            
            # åˆ›å»ºOllamaæœåŠ¡å®ä¾‹
            ollama_service = OllamaService(server_url)
            
            # è·å–æ¨¡å‹åç§°
            model_name = agent.get('model_name', 'llama2')
            
            # è·å–å‚æ•°é…ç½®
            parameters = agent.get('parameters', {})
            temperature = parameters.get('temperature', 0.7)
            max_tokens = parameters.get('max_tokens', 1000)
            
            # æ„å»ºå®Œæ•´çš„æç¤ºè¯ï¼ˆåŒ…å«ç³»ç»Ÿæç¤ºè¯ï¼‰
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            # è°ƒç”¨Ollamaç”Ÿæˆå›å¤
            response = ollama_service.generate_text(
                model_name=model_name,
                prompt=full_prompt,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            return response
            
        except Exception as e:
            print(f"âŒ è°ƒç”¨Ollamaç”Ÿæˆæ™ºèƒ½ä½“å›å¤å¤±è´¥: {str(e)}")
            raise e
    
    def _call_ollama_for_model(self, model: Dict[str, Any], user_prompt: str) -> str:
        """
        è°ƒç”¨Ollamaç”Ÿæˆæ¨¡å‹å›å¤
        
        Args:
            model: æ¨¡å‹é…ç½®ä¿¡æ¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            
        Returns:
            str: AIå›å¤å†…å®¹
        """
        try:
            # è·å–OllamaæœåŠ¡å™¨åœ°å€ï¼Œä¼˜å…ˆä½¿ç”¨æ¨¡å‹é…ç½®çš„server_url
            server_url = model.get('server_url', 'http://localhost:11434')
            print(f"ğŸ”§ ä½¿ç”¨Ollamaç”Ÿæˆ - æœåŠ¡å™¨: {server_url}")
            
            # åˆ›å»ºOllamaæœåŠ¡å®ä¾‹ï¼Œä½¿ç”¨æ¨¡å‹é…ç½®çš„æœåŠ¡å™¨åœ°å€
            from app.services.ollama import OllamaService
            ollama_service = OllamaService(server_url)
            
            # è·å–æ¨¡å‹åç§°
            model_name = model.get('name', 'llama2')
            
            # è·å–å‚æ•°é…ç½®
            parameters = model.get('parameters', {})
            temperature = parameters.get('temperature', 0.7)
            max_tokens = parameters.get('max_tokens', 1000)
            
            # è°ƒç”¨Ollamaç”Ÿæˆå›å¤
            response = ollama_service.generate_text(
                model_name=model_name,
                prompt=user_prompt,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            return response
            
        except Exception as e:
            print(f"âŒ è°ƒç”¨Ollamaç”Ÿæˆæ¨¡å‹å›å¤å¤±è´¥: {str(e)}")
            raise e
    
    def _call_ollama_stream_for_agent(self, agent: Dict[str, Any], system_prompt: str, user_prompt: str) -> Generator[str, None, None]:
        """
        è°ƒç”¨Ollamaæµå¼ç”Ÿæˆæ™ºèƒ½ä½“å›å¤
        
        Args:
            agent: æ™ºèƒ½ä½“é…ç½®ä¿¡æ¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            
        Yields:
            str: æµå¼å›å¤ç‰‡æ®µ
        """
        try:
            # è·å–OllamaæœåŠ¡å™¨åœ°å€
            server_url = agent.get('server_url', 'http://localhost:11434')
            print(f"ğŸ”§ è°ƒç”¨Ollamaæµå¼ç”Ÿæˆæ™ºèƒ½ä½“å›å¤ - æœåŠ¡å™¨: {server_url}")
            
            # åˆ›å»ºOllamaæœåŠ¡å®ä¾‹
            ollama_service = OllamaService(server_url)
            
            # è·å–æ¨¡å‹åç§°
            model_name = agent.get('model_name', 'llama2')
            
            # è·å–å‚æ•°é…ç½®
            parameters = agent.get('parameters', {})
            temperature = parameters.get('temperature', 0.7)
            max_tokens = parameters.get('max_tokens', 1000)
            
            # æ„å»ºå®Œæ•´çš„æç¤ºè¯ï¼ˆåŒ…å«ç³»ç»Ÿæç¤ºè¯ï¼‰
            full_prompt = f"{system_prompt}\n\n{user_prompt}"
            
            # è°ƒç”¨Ollamaæµå¼ç”Ÿæˆå›å¤
            for chunk in ollama_service.stream_text(
                model_name=model_name,
                prompt=full_prompt,
                temperature=temperature,
                max_tokens=max_tokens
            ):
                yield chunk
                
        except Exception as e:
            print(f"âŒ è°ƒç”¨Ollamaæµå¼ç”Ÿæˆæ™ºèƒ½ä½“å›å¤å¤±è´¥: {str(e)}")
            yield f"æŠ±æ­‰ï¼Œæ™ºèƒ½ä½“ {agent.get('name', 'æœªçŸ¥')} æš‚æ—¶æ— æ³•å“åº”ï¼š{str(e)}"
    
    def _call_ollama_stream_for_model(self, model: Dict[str, Any], user_prompt: str, show_thinking: bool = False) -> Generator[str, None, None]:
        """
        è°ƒç”¨Ollamaæµå¼ç”Ÿæˆæ¨¡å‹å›å¤
        
        Args:
            model: æ¨¡å‹é…ç½®ä¿¡æ¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            show_thinking: æ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
            
        Yields:
            str: æµå¼å›å¤ç‰‡æ®µ
        """
        try:
            # è·å–OllamaæœåŠ¡å™¨åœ°å€ï¼Œä¼˜å…ˆä½¿ç”¨æ¨¡å‹é…ç½®çš„server_url
            server_url = model.get('server_url', 'http://localhost:11434')
            print(f"ğŸ”§ ä½¿ç”¨Ollamaæµå¼ç”Ÿæˆ - æœåŠ¡å™¨: {server_url}")
            
            # åˆ›å»ºOllamaæœåŠ¡å®ä¾‹ï¼Œä½¿ç”¨æ¨¡å‹é…ç½®çš„æœåŠ¡å™¨åœ°å€
            from app.services.ollama import OllamaService
            ollama_service = OllamaService(server_url)
            
            # è·å–æ¨¡å‹åç§°
            model_name = model.get('name', 'llama2')
            
            # è·å–å‚æ•°é…ç½®
            parameters = model.get('parameters', {})
            temperature = parameters.get('temperature', 0.7)
            max_tokens = parameters.get('max_tokens', 1000)
            
            # è°ƒç”¨Ollamaæµå¼ç”Ÿæˆå›å¤
            for chunk in ollama_service.stream_text(
                model_name=model_name,
                prompt=user_prompt,
                temperature=temperature,
                max_tokens=max_tokens
            ):
                yield chunk
                
        except Exception as e:
            print(f"âŒ è°ƒç”¨Ollamaæµå¼ç”Ÿæˆæ¨¡å‹å›å¤å¤±è´¥: {str(e)}")
            yield f"æŠ±æ­‰ï¼Œæ¨¡å‹ {model.get('name', 'æœªçŸ¥')} æš‚æ—¶æ— æ³•å“åº”ï¼š{str(e)}" 