"""
èŠå¤©æœåŠ¡æ¨¡å—
å¤„ç†èŠå¤©ç›¸å…³çš„ä¸šåŠ¡é€»è¾‘
"""

import json
import time
from typing import Generator, Dict, Any
from app.services.ollama import OllamaService

class ChatService:
    def __init__(self):
        self.ollama_service = OllamaService()
    
    def generate_response_sync(self, conversation_id: str, agent: Dict[str, Any], user_message: str) -> str:
        """
        åŒæ­¥ç”ŸæˆAIå›å¤
        """
        try:
            # æ„å»ºç³»ç»Ÿæç¤ºè¯
            system_prompt = agent.get('config', {}).get('system_prompt', 'ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚')
            
            # æ„å»ºå®Œæ•´çš„æç¤ºè¯
            full_prompt = f"{system_prompt}\n\nç”¨æˆ·: {user_message}\nåŠ©æ‰‹: "
            
            # è°ƒç”¨OllamaæœåŠ¡ç”Ÿæˆå›å¤
            response = self.ollama_service.generate_text(
                model_name=agent.get('model_name', 'llama2'),
                prompt=full_prompt,
                temperature=agent.get('config', {}).get('temperature', 0.7),
                max_tokens=agent.get('config', {}).get('max_tokens', 1000)
            )
            
            return response.strip()
            
        except Exception as e:
            return f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼š{str(e)}"
    
    def stream_response(self, conversation_id: str, agent: Dict[str, Any], user_message: str) -> Generator[str, None, None]:
        """
        æµå¼ç”ŸæˆAIå›å¤
        """
        try:
            # æ„å»ºç³»ç»Ÿæç¤ºè¯
            system_prompt = agent.get('config', {}).get('system_prompt', 'ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚')
            
            # æ„å»ºå®Œæ•´çš„æç¤ºè¯
            full_prompt = f"{system_prompt}\n\nç”¨æˆ·: {user_message}\nåŠ©æ‰‹: "
            
            # è°ƒç”¨OllamaæœåŠ¡æµå¼ç”Ÿæˆå›å¤
            for chunk in self.ollama_service.stream_text(
                model_name=agent.get('model_name', 'llama2'),
                prompt=full_prompt,
                temperature=agent.get('config', {}).get('temperature', 0.7),
                max_tokens=agent.get('config', {}).get('max_tokens', 1000)
            ):
                yield chunk
                
        except Exception as e:
            yield f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼š{str(e)}"
    
    def generate_response_with_context(self, conversation_id: str, agent: Dict[str, Any], user_message: str, context_messages: list | None = None) -> str:
        """
        åŸºäºä¸Šä¸‹æ–‡ç”ŸæˆAIå›å¤
        """
        try:
            # æ„å»ºç³»ç»Ÿæç¤ºè¯
            system_prompt = agent.get('config', {}).get('system_prompt', 'ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚')
            
            # æ„å»ºå¯¹è¯å†å²
            conversation_history = ""
            if context_messages:
                for msg in context_messages[-5:]:  # åªä½¿ç”¨æœ€è¿‘5æ¡æ¶ˆæ¯ä½œä¸ºä¸Šä¸‹æ–‡
                    role = "ç”¨æˆ·" if msg['type'] == 'user' else "åŠ©æ‰‹"
                    conversation_history += f"{role}: {msg['content']}\n"
            
            # æ„å»ºå®Œæ•´çš„æç¤ºè¯
            full_prompt = f"{system_prompt}\n\n{conversation_history}ç”¨æˆ·: {user_message}\nåŠ©æ‰‹: "
            
            # è°ƒç”¨OllamaæœåŠ¡ç”Ÿæˆå›å¤
            response = self.ollama_service.generate_text(
                model_name=agent.get('model_name', 'llama2'),
                prompt=full_prompt,
                temperature=agent.get('config', {}).get('temperature', 0.7),
                max_tokens=agent.get('config', {}).get('max_tokens', 1000)
            )
            
            return response.strip()
            
        except Exception as e:
            return f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼š{str(e)}"
    
    def stream_response_with_context(self, conversation_id: str, agent: Dict[str, Any], user_message: str, context_messages: list | None = None) -> Generator[str, None, None]:
        """
        åŸºäºä¸Šä¸‹æ–‡æµå¼ç”ŸæˆAIå›å¤
        """
        try:
            # æ„å»ºç³»ç»Ÿæç¤ºè¯
            system_prompt = agent.get('config', {}).get('system_prompt', 'ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚')
            
            # æ„å»ºå¯¹è¯å†å²
            conversation_history = ""
            if context_messages:
                for msg in context_messages[-5:]:  # åªä½¿ç”¨æœ€è¿‘5æ¡æ¶ˆæ¯ä½œä¸ºä¸Šä¸‹æ–‡
                    role = "ç”¨æˆ·" if msg['type'] == 'user' else "åŠ©æ‰‹"
                    conversation_history += f"{role}: {msg['content']}\n"
            
            # æ„å»ºå®Œæ•´çš„æç¤ºè¯
            full_prompt = f"{system_prompt}\n\n{conversation_history}ç”¨æˆ·: {user_message}\nåŠ©æ‰‹: "
            
            # è°ƒç”¨OllamaæœåŠ¡æµå¼ç”Ÿæˆå›å¤
            for chunk in self.ollama_service.stream_text(
                model_name=agent.get('model_name', 'llama2'),
                prompt=full_prompt,
                temperature=agent.get('config', {}).get('temperature', 0.7),
                max_tokens=agent.get('config', {}).get('max_tokens', 1000)
            ):
                yield chunk
                
        except Exception as e:
            yield f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼š{str(e)}" 
    
    def generate_model_response_sync(self, conversation_id: str, model: Dict[str, Any], user_message: str) -> str:
        """
        åŒæ­¥ç”Ÿæˆæ¨¡å‹å›å¤
        """
        try:
            print(f"ğŸ”§ ç”Ÿæˆæ¨¡å‹å›å¤ - æ¨¡å‹: {model.get('name', 'æœªçŸ¥')}, æä¾›å•†: {model.get('provider', 'æœªçŸ¥')}")
            
            if model.get('provider') == 'ollama':
                # ä»æ¨¡å‹é…ç½®ä¸­è·å–å‚æ•°
                parameters = model.get('parameters', {})
                temperature = parameters.get('temperature', 0.7)
                max_tokens = parameters.get('max_tokens', 1000)
                
                # è·å–OllamaæœåŠ¡å™¨åœ°å€
                server_url = model.get('api_base', 'http://localhost:11434')
                print(f"ğŸ”§ ä½¿ç”¨Ollamaç”Ÿæˆ - æœåŠ¡å™¨: {server_url}, æ¸©åº¦: {temperature}, æœ€å¤§token: {max_tokens}")
                
                # åˆ›å»ºOllamaæœåŠ¡å®ä¾‹
                from app.services.ollama import OllamaService
                ollama_service = OllamaService(server_url)
                
                response = ollama_service.generate_text(
                    model_name=model.get('name', 'llama2'),
                    prompt=user_message,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                return response.strip()
            else:
                return f"æ¨¡å‹ {model.get('name', 'æœªçŸ¥')} çš„å›å¤ï¼š{user_message}"
                
        except Exception as e:
            print(f"âŒ æ¨¡å‹å›å¤ç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"æŠ±æ­‰ï¼Œæ¨¡å‹ {model.get('name', 'æœªçŸ¥')} é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼š{str(e)}"
    
    def stream_model_response(self, conversation_id: str, model: Dict[str, Any], user_message: str) -> Generator[str, None, None]:
        """
        æµå¼ç”Ÿæˆæ¨¡å‹å›å¤
        """
        try:
            print(f"ğŸ”§ æµå¼ç”Ÿæˆæ¨¡å‹å›å¤ - æ¨¡å‹: {model.get('name', 'æœªçŸ¥')}, æä¾›å•†: {model.get('provider', 'æœªçŸ¥')}")
            
            if model.get('provider') == 'ollama':
                # ä»æ¨¡å‹é…ç½®ä¸­è·å–å‚æ•°
                parameters = model.get('parameters', {})
                temperature = parameters.get('temperature', 0.7)
                max_tokens = parameters.get('max_tokens', 1000)
                
                # è·å–OllamaæœåŠ¡å™¨åœ°å€
                server_url = model.get('api_base', 'http://localhost:11434')
                print(f"ğŸ”§ ä½¿ç”¨Ollamaæµå¼ç”Ÿæˆ - æœåŠ¡å™¨: {server_url}, æ¸©åº¦: {temperature}, æœ€å¤§token: {max_tokens}")
                
                # åˆ›å»ºOllamaæœåŠ¡å®ä¾‹
                from app.services.ollama import OllamaService
                ollama_service = OllamaService(server_url)
                
                # æ„å»ºåŒ…å«æ€è€ƒè¿‡ç¨‹çš„æç¤ºè¯
                thinking_prompt = f"""è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›å¤ï¼š

<think>
åœ¨è¿™é‡Œåˆ†æé—®é¢˜ï¼Œè€ƒè™‘å„ç§å¯èƒ½æ€§
</think>

åœ¨è¿™é‡Œç»™å‡ºæœ€ç»ˆçš„å›ç­”

ç”¨æˆ·é—®é¢˜ï¼š{user_message}"""
                
                for chunk in ollama_service.stream_text(
                    model_name=model.get('name', 'llama2'),
                    prompt=thinking_prompt,
                    temperature=temperature,
                    max_tokens=max_tokens
                ):
                    yield chunk
            else:
                yield f"æ¨¡å‹ {model.get('name', 'æœªçŸ¥')} çš„å›å¤ï¼š{user_message}"
                
        except Exception as e:
            print(f"âŒ æµå¼æ¨¡å‹å›å¤ç”Ÿæˆå¤±è´¥: {str(e)}")
            yield f"æŠ±æ­‰ï¼Œæ¨¡å‹ {model.get('name', 'æœªçŸ¥')} é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼š{str(e)}" 